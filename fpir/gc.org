* On Garbage Collection For Lean Systems
The question of how memory management should work is ever-present in
language design. How a language thinks about its memory and how the
responsibilities governing the correct semantics of a program are
divided between the language, the compiler if there is one, and the
programmer have a profound effect on the feel of a language. These
decisions direct what will become natural and idiomatic, and go a long
way on defining the set of things a programmer might /want/ to write
in the language.

The most significant single determination when talking about memory is
whether the language features and/or encourages the use of garbage
collection. In general this choice is one of trading some control and
some efficiency (often surrendering the former leads to the latter)
for conceptual niceness. When the programmer no longer needs to
concern themselves over lifetimes and ownership of her data, she can
think directly and undistractedly about the actual core problem her
program is to solve. The question of if that clarity is worth some
runtime cost is deferred to every other corner of the internet arguing
over programming languages. Instead, here I am concerned with what
garbage collection can offer us as low level programmers.

I want to spend some time talking about what my implementation of
garbage collection looks like, what sorts of problems and decisions it
lead my overall design towards, and why I feel that the not
insignificant costs it incurs are worth it.

My goal for this language is to build as nice abstractions as I can
with as little base as possible. By that I mean that:
- I should not expect anything from the underlying system. This means
  that the language should rest naturally on bare metal, and should be
  almost trivially portable.
- The language should make it both possible and desirable to program
  at a high level of abstraction. If my problem lends itself naturally
  to some elegant functional solution, I should solve it as such
  without the knowledge of my proximity to the hardware directing my
  programming style towards something crunchier.
The astute reader will notice that runtime efficiency is not on that
list. While efficiency concerns are not entirely waylaid, they are not
my focus here.

** Implementing Garbage Collection
Having decided that I want garbage collection in my language, and with
the knowledge that my language currently operates over cons cells as
its sole data structure, how should I go about implementing a garbage
collector that makes sense for a baremetal environment?

With the goals of my language and simplicity top of mind, I chose to
implement a semi-space allocate. Cheney's algorithm provides a simple
and reliable implementation. A bump allocator ensures that the
overhead incurred for each allocation is low, and a large total memory
size compared to the working set ensures that the cost of garbage
collection when it happens is amortized.

While it is certainly a large concession to have only half of your
total memory available at a time, this sort of semispace design shines
in simplicity and zero overhead. The mark and sweep garbage collector
means that allocations (which are already type-tagged for other
reasons), need zero additional data for them garbage-collectable. No
generational tags, no table of live or dead blocks, no linked list
stored in free nodes. The program simply has some set of roots, and
every cons cell reachable from a root is still valid. The natural
implementation with forwarding pointers even handles cyclical data
structures cleanly with no additional logic. Garbage collection
absolutely shreds any possible cache locality by being a breadth first
search, but a program composed of exclusively cons cells is not
exactly dense to begin with.

Having extolled the virtues of this system enough, a question remains:
When does garbage collection happen? It turns out this question is not
quite as simple to answer as it might sound. An initial guiding
decision could be that we don't want to claim an out of memory error
if we don't really have one...

*** The Problemâ„¢
The first thing that comes to mind is simply check each time we
would allocate if it would allocate past the end of the heap, and if
so perform garbage collection first. If we would still overflow the
heap after garbage collection, then we truly have run out of memory.

However, calling collect in new_cons when we would overflow is
unreliable, as there may be valid cons cells that are not yet
reachable from the roots. These cells will not be copied into the new
space, and thus will not be valid into the future. Furthermore, even
if they were copied, if they are not reachable by the root, we can't
be sure that all the locations with pointers to the cells in question
will be updated with the new address. A natural case where this arises
is building some multi-cons structure before inserting it into a
larger data structure. Perhaps a 4 leaf balanced binary tree. If one
allocates the leaf nodes first and the new_cons call to allocate the
root triggers garbage collection, we need to be sure that the leaf
nodes are still alive and coherent.

**** Call Collect Somewhere Else?
Ideologically I don't want to, since the optimum solution is to only
call collect when you need to, since it is more efficient as a batch
operation than run more frequently, even if each invocation does less
work.

Furthermore there is a correctness problem which is discussed below.

**** A Grace Period Perhaps?
Fix some finite natural K. Change the new_cons collect check to
activate any time there are less than K cells left to allocate in the
current space. There exists a global flag that enables and disables
this check entirely, effectively turning off garbage collection. When
collection it re-enabled, the check is performed without waiting for
the next call to new_cons. The program is correct with effective
memory (M - K) if it upholds the following invariant:

The program disables garbage collection for sections in which new
cells are not reachable from the roots before the next call to
new_cons, and all such sections call new_cons no more than K
times.

I mean that the program is correct in that a program that upholds the
invariant under this scheme will never fail because of a lack of
memory unless the same program would fail under a hypothetical perfect
garbage collector / allocator which will only fail to allocate more
memory if the entirety of memory is in use.

In more friendly terms, when the programmer chooses, she can go up to
K allocations without having them be reachable from a root before
having to include them and re-enable garbage collection. While not a
natural solution, these sections should be small and this would give
the implementer the freedom to make the locally best decision about
how to handle allocations and build small structures.

**** The Snag
The issue is that we can't uphold the invariant in one specific
scenario. That scenario is the read call. When ingesting input from
the user, read needs to produce arbitrarily long lists, which mean
that any single call to read can recursively produce an unbounded
number of calls to new_cons.

The solution is to introduce some new root or set of roots that can
capture the work-in-progress list that read is building. The question
is then how to efficiently build up lists under the restriction that
every addition to the list must be navigated to starting at some root,
since a local reference (something like a variable in C) to a cons
cell is not guaranteed to remain valid through a second new_cons call
(or K new_cons calls).

We also can't use a naive solution like inserting at the tail starting
from the head (root) every time, since the read call producing lists
is at the heart of ingesting new code. If the program has to run a n^2
algorithm over the length of each top-level form, any semi-complex
code will be unacceptably slow to load. Admittedly code loading speed
is not the most critical metric, but that's no excuse to introduce
quadratic runtime.

**** Resignation
So it appears that any finite grace period won't cut it, and some sort
of infinite grace period seems like asking for trouble. Lets commit to
garbage collection being a very real possibility as a side effect of
*every* new_cons call. At least we have the consolation that we are
really using every last cons cell in each semi-space. What does it
look like to program in that world?

** The Consequences of Our Actions
Having committed to the above solution, we are armed with a functional
memory allocator and automatic garbage collector so long as we the
programmer remember to uphold the contract that all valid data
descends from a root, and /only/ such descendants are valid across a
new_cons call. Even worse, cells that we know will survive through a
collection, like longstanding members of the root environment, will
change address during garbage collection. This means that the
programmer can't even hold a pointer to something that is guaranteed
to survive through a call.

For a language in which the only non-stack data structure is a cons
cell, one ends up calling new_cons all the time. In addition, the core
loop that drives the inner language often does small allocations, for
instance when performing a procedure call. When writing the various
primitives and subroutines in C, this means that one can't hold a
pointer to the working set of cells if part of the work to do involves
allocating. While a C variable pointing to the data of interest is the
sensible way to access it, as soon as one might want to do work it can
become invalid.

This leads to a very careful dance when programming primitives and
aspects of the core loop in C. Often placing the temporary or
ephemeral data onto the stack and then operating there is the most
reasonable method, as the stack is always counted as a root. Even this
is not a complete solution however, since when building some data
structures, one has to allocate cells that might not yet point to
valid data. To solve this the garbage collector silently ignores null
pointers (different than a pointer to nil). Null pointers are not a
valid expression in the internal semantics of the language, but this
protects against a case in which a new cell is allocated that only has
one valid child, and the next allocation would fill in said child. An
intervening collection can be made safe by initializing every new cons
cell to a pair of nulls.

Even innocuous assumptions are now dangerous. We must compile with
strictly zero optimizations, since even something simple like reading
into a register instead of reading from memory every time could
produce these sorts of errors. It is impractical to write scoped and
hierarchically structured C, since any subcall that might allocate
would invalidate the C stack locals of the entire call stack. Passing
pointers as C arguments very quickly builds up assumptions that can
only be only be checked and ensured by the programmer, if she
remembers to.

All of these factors steer us to the C implementation being extremely
flat. By that I mean that the C call stack never rises above 3 or 4
frames in height, and the conception of the C program is not a program
that implements the semantics of the language, but rather a program
that implements a finite state machine that operates over the
structured data of the inner target language program to implicitly
produce the semantics of the target language. The state of the C
program represents the state of the inner program only in that the
roots are held as static C variables. Inspecting the locals and call
stack of the C program would provide you with almost zero data about
what is going on in the inner program. This is makes debugging all the
more painful.

At this point, a reasonable question might be "Why did you write this
in C at all?" It's certainly true that my internal model of what is
going on was almost that of assembly. The copious conversions between
unsigned integers and pointers should make that quite apparent. The
short answer is that I wrote it in C so that I could run it both on my
x86_64 laptop and on a RISC-V machine, which is were my longer term
interests for this project lie. By leaning very heavily on both the C
preprocessor and the fact that at the end of the day, the strength of
C is nothing more than that it maps very straightforwardly onto
machine instructions (for sane ISAs anyway). Writing C is the closest
thing to being able to write ASM for RISC-V and x86_64 at the same
time. And who likes making linux system calls manually?

With this I believe I have justified why the code looks the way it
does. And perhaps more importantly, what I and some hypothetical
observer could learn from it.
